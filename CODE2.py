from __future__ import annotations
import os, json, random
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Set, Tuple

# --- –±–∞–∑–æ–≤—ñ ---
import numpy as np

# --- Tkinter / matplotlib (—Å–ø–µ—Ä—à—É –±–µ–∫–µ–Ω–¥, –ø–æ—Ç—ñ–º pyplot) ---
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import matplotlib
matplotlib.use("TkAgg")
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.pyplot as plt

# --- Transformers / torch ---
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
except Exception:
    AutoTokenizer = AutoModel = torch = None

# --- SentenceTransformers (SBERT) ---
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

# --- FAISS / sklearn ---
try:
    import faiss
except Exception:
    faiss = None

try:
    from sklearn.neighbors import NearestNeighbors
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except Exception:
    NearestNeighbors = TfidfVectorizer = cosine_similarity = None

# --- joblib –¥–ª—è –∫–µ—à—É TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ ---
try:
    from joblib import dump as joblib_dump, load as joblib_load
except Exception:
    joblib_dump = joblib_load = None

# =======================
# –ö–æ–Ω—Ñ—ñ–≥
# =======================
DATA_DIR = Path("MSRCaseStudy/ganttJSON")    # —Ç—É—Ç –ª–µ–∂–∞—Ç—å methods.json, requirements.json, traces.json
CACHE_DIR = Path("emb_cache")

MODEL_NAME_CODEBERT = "microsoft/codebert-base"
MODEL_NAME_SBERT = "sentence-transformers/all-MiniLM-L6-v2"  
USE_FAISS = True
TOP_K = 500
MAX_LEN = 256
RANDOM_SEED = 42
SIM_THRESHOLD = 0  # –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å

for d in [DATA_DIR, CACHE_DIR]:
    d.mkdir(exist_ok=True)

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =======================
# –ú–æ–¥–µ–ª—ñ –¥–∞–Ω–∏—Ö
# =======================
@dataclass
class Method:
    id: int
    fullmethod: str
    classname: Optional[str] = None
    classid: Optional[int] = None
    sourcecode: Optional[str] = None

@dataclass
class Requirement:
    id: int
    requirementname: str
    text: Optional[str] = None

# =======================
# IO
# =======================
def load_json(path: Path):
    if not path.exists():
        return None
    with path.open("r", encoding="utf-8") as f:
        s = f.read().strip()
        return json.loads(s) if s else None

# =======================
# –ü–∞—Ä—Å–µ—Ä–∏ –¥–∞—Ç–∞—Å–µ—Ç—É
# =======================
def build_methods(methods_raw: List[Dict[str, Any]]) -> List[Method]:
    out = []
    for m in methods_raw or []:
        try:
            mid = int(m.get("id"))
        except Exception:
            continue
        classid = m.get("classid") or m.get("ownerclassid")
        try:
            classid = int(classid) if classid is not None else None
        except Exception:
            classid = None
        out.append(Method(
            id=mid,
            fullmethod=m.get("fullmethod") or m.get("methodname") or "",
            classname=m.get("classname") or None,
            classid=classid,
            sourcecode=m.get("method") or m.get("sourcecode") or ""
        ))
    # —É–Ω—ñ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞ id
    return list({m.id: m for m in out}.values())

def build_requirements(reqs_raw: List[Dict[str, Any]]) -> List[Requirement]:
    out = []
    for r in reqs_raw or []:
        try:
            rid = int(r.get("id"))
        except Exception:
            continue
        text = r.get("text") or r.get("description") or "(–Ω–µ–º–∞—î —Ç–µ–∫—Å—Ç—É)"
        out.append(Requirement(
            id=rid,
            requirementname=r.get("requirementname") or f"R{rid}",
            text=str(text).strip()
        ))
    return list({r.id: r for r in out}.values())

def build_gold_truth(traces_raw: List[Dict[str, Any]]) -> Dict[str, Set[int]]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î: requirementname -> –º–Ω–æ–∂–∏–Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö method_id
    (T / E  –≤ goldfinal/label)
    """
    truth: Dict[str, Set[int]] = {}
    POS = {"T", "E",}
    for t in traces_raw or []:
        req_name = str(t.get("requirement"))
        lab = str(t.get("goldfinal", t.get("label", "F"))).upper()
        if req_name and lab in POS:
            try:
                mid = int(t.get("methodid"))
            except Exception:
                continue
            truth.setdefault(req_name, set()).add(mid)
    return truth

# =======================
# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç—É
# =======================
def prepare_method_text(m: Method) -> str:
    return "\n".join(filter(None, [m.fullmethod, m.classname, m.sourcecode]))

def prepare_requirement_text(r: Requirement) -> str:
    return f"{r.requirementname}\n{r.text or ''}".strip()

# =======================
# –ï–º–±–µ–¥–¥–µ—Ä–∏
# =======================
class CodeBertEmbedder:
    def __init__(self, model_name=MODEL_NAME_CODEBERT, max_length=MAX_LEN, local_model_dir: Optional[str] = None):
        if AutoModel is None or torch is None:
            raise ImportError("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å transformers —Ç–∞ torch –¥–ª—è CodeBERT")
        os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
        os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
        try:
            torch.set_num_threads(1)
        except Exception:
            pass

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.max_length = max_length
        self.tokenizer, self.model = self._safe_load(model_name, local_model_dir)

        if self.device == "cuda":
            try:
                self.model.to(self.device, dtype=torch.float16)
            except Exception:
                self.model.to(self.device)
        else:
            self.model.to(self.device)
        self.model.eval()
        print(f"‚úÖ CodeBERT –≥–æ—Ç–æ–≤–∏–π. –ü—Ä–∏—Å—Ç—Ä—ñ–π: {self.device}")

    def _safe_load(self, model_name: str, local_model_dir: Optional[str]):
        from transformers import AutoTokenizer, AutoModel

        if local_model_dir:
            print(f"‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CodeBERT –∑ –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–∞–ø–∫–∏: {local_model_dir}")
            tok = AutoTokenizer.from_pretrained(local_model_dir, use_fast=True, local_files_only=True)
            mdl = AutoModel.from_pretrained(local_model_dir, local_files_only=True)
            return tok, mdl

        # 1) –æ–Ω–ª–∞–π–Ω, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∫–µ—à
        try:
            print("üß† –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CodeBERT (–æ–Ω–ª–∞–π–Ω, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∫–µ—à)...")
            tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
            mdl = AutoModel.from_pretrained(model_name)
            return tok, mdl
        except Exception as e1:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∫–µ—à): {e1}")


    def encode(self, texts: List[str], cache_file: Path) -> np.ndarray:
        print(f"üîπ CodeBERT embeddings: {len(texts)} –µ–ª–µ–º–µ–Ω—Ç—ñ–≤; –∫–µ—à: {cache_file.name}")
        if cache_file.exists():
            print("‚ö° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CodeBERT –∑ –∫–µ—à—É‚Ä¶")
            return np.load(cache_file)

        embs = []
        batch_size = 32
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            enc = self.tokenizer(batch, padding=True, truncation=True,
                                 max_length=self.max_length, return_tensors="pt")
            with torch.no_grad():
                inputs = {k: v.to(self.device) for k, v in enc.items()}
                out = self.model(**inputs)
                last_hidden = out.last_hidden_state
                mask = enc["attention_mask"].to(self.device).unsqueeze(-1)
                v = (last_hidden * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)
                embs.append(v.detach().cpu().numpy())
            print(f"   üü¢ {min(i + batch_size, len(texts))}/{len(texts)}")

        arr = np.vstack(embs).astype(np.float32)
        arr /= np.linalg.norm(arr, axis=1, keepdims=True) + 1e-8
        np.save(cache_file, arr)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ CodeBERT –∫–µ—à: {cache_file.name}")
        return arr

class SbertEmbedder:
    def __init__(self, model_name=MODEL_NAME_SBERT, max_length=MAX_LEN):
        if SentenceTransformer is None:
            raise ImportError("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å sentence-transformers –¥–ª—è SBERT (all-mpnet-base-v2)")
        os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
        device = "cuda" if torch is not None and torch.cuda.is_available() else "cpu"
        self.max_length = max_length
        self.model = SentenceTransformer(model_name, device=device)
        print(f"‚úÖ SBERT ({model_name}) –≥–æ—Ç–æ–≤–∏–π. –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

    def encode(self, texts: List[str], cache_file: Path) -> np.ndarray:
        print(f"üîπ SBERT embeddings: {len(texts)} –µ–ª–µ–º–µ–Ω—Ç—ñ–≤; –∫–µ—à: {cache_file.name}")
        if cache_file.exists():
            print("‚ö° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è SBERT –∑ –∫–µ—à—É‚Ä¶")
            return np.load(cache_file)
        truncated = [
            t[:self.max_length]  
            for t in texts
        ]
        embs = self.model.encode(
            truncated,
            batch_size=32,
            convert_to_numpy=True,
            show_progress_bar=True,
            normalize_embeddings=True,
        ).astype(np.float32)

        np.save(cache_file, embs)
        print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ SBERT –∫–µ—à: {cache_file.name}")
        return embs


# =======================
# –Ü–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä (–¥–ª—è SBERT / CodeBERT)
# =======================
class Indexer:
    def __init__(self, dim, use_faiss=True):
        self.use_faiss = bool(use_faiss and (faiss is not None))
        if not self.use_faiss and NearestNeighbors is None:
            raise ImportError("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å faiss-cpu –∞–±–æ scikit-learn")
        self.id_map: List[int] = []
        self.index = faiss.IndexFlatIP(dim) if self.use_faiss else NearestNeighbors(metric="cosine")

    def fit(self, vectors: np.ndarray, ids: List[int]):
        if self.use_faiss:
            faiss.normalize_L2(vectors)
            self.index.add(vectors)
        else:
            self.index.fit(vectors)
        self.id_map = ids

    def search(self, q_vecs: np.ndarray, k: int) -> List[List[Tuple[int, float]]]:
        if self.use_faiss:
            faiss.normalize_L2(q_vecs)
            D, I = self.index.search(q_vecs, k)
            return [[(self.id_map[j], float(D[i][n])) for n, j in enumerate(I[i])] for i in range(len(q_vecs))]
        else:
            dist, ind = self.index.kneighbors(q_vecs, n_neighbors=k)
            return [[(self.id_map[j], 1.0 - float(dist[i][n])) for n, j in enumerate(ind[i])] for i in range(len(q_vecs))]

# =======================
# –ú–µ—Ç—Ä–∏–∫–∏ (–ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –æ–¥–Ω—ñ—î—ó –≤–∏–º–æ–≥–∏)
# =======================
def compute_local_metrics(pred_ids: List[int], gold_ids: Set[int]) -> Dict[str, float]:
   
    if not pred_ids or not gold_ids:
        return {
            "Precision": 0.0,
            "Recall": 0.0,
            "F1": 0.0,
            "MAP": 0.0,
            "MRR": 0.0,
        }

    hits_total = sum(1 for p in pred_ids if p in gold_ids)
    precision = hits_total / len(pred_ids)
    recall = hits_total / len(gold_ids)
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0

    # MRR
    mrr = 0.0
    for i, pid in enumerate(pred_ids, 1):
        if pid in gold_ids:
            mrr = 1.0 / i
            break

    # AP
    ap_sum, rel_seen = 0.0, 0
    for i, pid in enumerate(pred_ids, 1):
        if pid in gold_ids:
            rel_seen += 1
            ap_sum += rel_seen / i
    map_ = ap_sum / len(gold_ids) if len(gold_ids) else 0.0


    return {
        "Precision": float(precision),
        "Recall": float(recall),
        "F1": float(f1),
        "MAP": float(map_),
        "MRR": float(mrr),
    }

# =======================
# TF-IDF pipeline
# =======================
def run_tfidf_pipeline(methods: List[Method],
                       requirements: List[Requirement],
                       top_k: int) -> Dict[int, List[Tuple[int, float]]]:
    if TfidfVectorizer is None or cosine_similarity is None:
        raise ImportError("–î–ª—è TF-IDF –ø–æ—Ç—Ä—ñ–±–µ–Ω scikit-learn")

    print("\n==========================")
    print("   –ú–û–î–ï–õ–¨: TF-IDF")
    print("==========================")

    method_texts = [prepare_method_text(m) for m in methods]
    req_texts = [prepare_requirement_text(r) for r in requirements]

    vec_path = CACHE_DIR / "tfidf_vectorizer.joblib"

    if vec_path.exists() and joblib_load is not None:
        print("‚ö° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –∑ –∫–µ—à—É‚Ä¶")
        vectorizer: TfidfVectorizer = joblib_load(vec_path)
    else:
        print("üß† –ù–∞–≤—á–∞–Ω–Ω—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞‚Ä¶")
        vectorizer = TfidfVectorizer(
            max_features=50000,
            ngram_range=(1, 2),
            lowercase=True
        )
        vectorizer.fit(method_texts + req_texts)
        if joblib_dump is not None:
            joblib_dump(vectorizer, vec_path)
            print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —É {vec_path.name}")
        else:
            print("‚ö†Ô∏è joblib –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π ‚Äì –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –±—É–¥–µ –∑–∞–∫–µ—à–æ–≤–∞–Ω–æ.")

    print("üîπ –û–±—á–∏—Å–ª–µ–Ω–Ω—è TF-IDF –º–∞—Ç—Ä–∏—Ü—å‚Ä¶")
    M = vectorizer.transform(method_texts)   # (n_methods, d)
    R = vectorizer.transform(req_texts)      # (n_reqs, d)

    print("üîπ –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ—ó —Å—Ö–æ–∂–æ—Å—Ç—ñ TF-IDF (–º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–æ)‚Ä¶")
    sims = cosine_similarity(R, M)           # (n_reqs, n_methods)

    retrieved: Dict[int, List[Tuple[int, float]]] = {}
    n_methods = sims.shape[1]

    for i, r in enumerate(requirements):
        row = sims[i]
        k = min(top_k, n_methods)
        if k <= 0:
            retrieved[r.id] = []
            continue
        # —ñ–Ω–¥–µ–∫—Å–∏ —Ç–æ–ø-k
        top_idx = np.argpartition(row, -k)[-k:]
        top_idx_sorted = top_idx[np.argsort(-row[top_idx])]
        retrieved[r.id] = [(methods[j].id, float(row[j])) for j in top_idx_sorted if row[j] >= SIM_THRESHOLD]


    print("‚úÖ TF-IDF: –ø–æ–±—É–¥–æ–≤–∞–Ω–æ Top-K —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å—ñ—Ö –≤–∏–º–æ–≥.")
    return retrieved

# =======================
# SBERT pipeline
# =======================
def run_sbert_pipeline(methods: List[Method],
                       requirements: List[Requirement],
                       top_k: int) -> Dict[int, List[Tuple[int, float]]]:
    print("\n==========================")
    print("   –ú–û–î–ï–õ–¨: SBERT (all-MiniLM-L6-v2)")
    print("==========================")

    embedder = SbertEmbedder(MODEL_NAME_SBERT)
    method_texts = [prepare_method_text(m) for m in methods]
    req_texts = [prepare_requirement_text(r) for r in requirements]

    m_emb = embedder.encode(method_texts, CACHE_DIR / "sbert_methods.npy")
    r_emb = embedder.encode(req_texts, CACHE_DIR / "sbert_requirements.npy")

    indexer = Indexer(m_emb.shape[1], USE_FAISS)
    indexer.fit(m_emb, [m.id for m in methods])
    lists = indexer.search(r_emb, top_k)

    # –í—Å—Ç–∞–≤–ª—è—î–º–æ —Ñ—ñ–ª—å—Ç—Ä –∑–∞ –ø–æ—Ä–æ–≥–æ–º
    filtered_lists = []
    for res in lists:
        filtered = [(mid, score) for mid, score in res if score >= SIM_THRESHOLD]
        filtered_lists.append(filtered)

    retrieved = {r.id: res for r, res in zip(requirements, filtered_lists)}

    print("‚úÖ SBERT: –ø–æ–±—É–¥–æ–≤–∞–Ω–æ Top-K —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å—ñ—Ö –≤–∏–º–æ–≥.")
    return retrieved

# =======================
# CodeBERT pipeline
# =======================
def run_codebert_pipeline(methods: List[Method],
                          requirements: List[Requirement],
                          top_k: int) -> Dict[int, List[Tuple[int, float]]]:
    print("\n==========================")
    print("   –ú–û–î–ï–õ–¨: CodeBERT")
    print("==========================")

    embedder = CodeBertEmbedder()
    method_texts = [prepare_method_text(m) for m in methods]
    req_texts = [prepare_requirement_text(r) for r in requirements]

    m_emb = embedder.encode(method_texts, CACHE_DIR / "codebert_methods.npy")
    r_emb = embedder.encode(req_texts, CACHE_DIR / "codebert_requirements.npy")

    indexer = Indexer(m_emb.shape[1], USE_FAISS)
    indexer.fit(m_emb, [m.id for m in methods])
    lists = indexer.search(r_emb, top_k)

    # –§—ñ–ª—å—Ç—Ä –∑–∞ –ø–æ—Ä–æ–≥–æ–º
    filtered_lists = []
    for res in lists:
        filtered = [(mid, score) for mid, score in res if score >= SIM_THRESHOLD]
        filtered_lists.append(filtered)

    retrieved = {r.id: res for r, res in zip(requirements, filtered_lists)}

    print("‚úÖ CodeBERT: –ø–æ–±—É–¥–æ–≤–∞–Ω–æ Top-K —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å—ñ—Ö –≤–∏–º–æ–≥.")
    return retrieved

# =======================
# –ì–ª–æ–±–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–æ–¥–µ–ª—ñ
# =======================
def compute_global_metrics_for_model(
    model_name: str,
    retrieved_map: Dict[int, List[Tuple[int, float]]],
    requirements: List[Requirement],
    gold_truth: Dict[str, Set[int]],
    top_k: int
) -> Dict[str, float]:
    sum_metrics = {
        "Precision": 0.0,
        "Recall": 0.0,
        "F1": 0.0,
        "MAP": 0.0,
        "MRR": 0.0,
    }
    cnt = 0

    for r in requirements:
        gold_ids = gold_truth.get(r.requirementname, set())
        pred_pairs = retrieved_map.get(r.id, [])
        pred_ids = [mid for (mid, _) in pred_pairs[:top_k]]

        if not pred_ids or not gold_ids:
            continue

        m = compute_local_metrics(pred_ids, gold_ids)
        for k in sum_metrics:
            sum_metrics[k] += m[k]
        cnt += 1

    if cnt == 0:
        print(f"‚ùå {model_name}: –Ω–µ–º–∞—î –≤–∏–º–æ–≥ –∑ gold-—Ç—Ä–∞—Å–∞–º–∏.")
        return {k: 0.0 for k in sum_metrics}

    avg = {k: v / cnt for k, v in sum_metrics.items()}

    print(f"\nüìä –ü—ñ–¥—Å—É–º–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª—ñ {model_name} (—É—Å–µ—Ä–µ–¥–Ω–µ–Ω–æ –ø–æ {cnt} –≤–∏–º–æ–≥–∞—Ö, Top-{top_k}):")
    for k, v in avg.items():
        print(f"  {k:12s} = {v:.4f}")

    return avg

# =======================
# –ü–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ
# =======================
def run_model_pipeline(
    model_name: str,
    methods: List[Method],
    requirements: List[Requirement],
    gold_truth: Dict[str, Set[int]],
    top_k: int
) -> Tuple[Dict[int, List[Tuple[int, float]]], Dict[str, float]]:

    if model_name == "TF-IDF":
        retrieved = run_tfidf_pipeline(methods, requirements, top_k)
    elif model_name == "SBERT":
        retrieved = run_sbert_pipeline(methods, requirements, top_k)
    elif model_name == "CodeBERT":
        retrieved = run_codebert_pipeline(methods, requirements, top_k)
    else:
        raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å: {model_name}")

    avg_metrics = compute_global_metrics_for_model(model_name, retrieved, requirements, gold_truth, top_k)
    return retrieved, avg_metrics

# =======================
# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É (—Å–ø—ñ–ª—å–Ω–µ –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π)
# =======================
def load_dataset(data_dir: Path):
    data = {k: load_json(data_dir / f"{k}.json") for k in ["methods", "requirements", "traces"]}
    if not all(data.values()):
        miss = [k for k, v in data.items() if not v]
        raise FileNotFoundError(f"–í—ñ–¥—Å—É—Ç–Ω—ñ —Ñ–∞–π–ª–∏: {miss}")
    methods = build_methods(data["methods"])
    requirements = build_requirements(data["requirements"])
    gold_truth = build_gold_truth(data["traces"])
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç: methods={len(methods)}, requirements={len(requirements)}, traces={len(data['traces'])}")
    return methods, requirements, gold_truth, data["traces"]

# =======================
# GUI (—ñ–∑ –ø–µ—Ä–µ–º–∏–∫–∞—á–µ–º –º–æ–¥–µ–ª–µ–π)
# =======================
def launch_gui(
    requirements: List[Requirement],
    methods: List[Method],
    gold_truth: Dict[str, Set[int]],
    traces: List[Dict[str, Any]],
    retrieved_by_model: Dict[str, Dict[int, List[Tuple[int, float]]]],
    top_k_default: int = TOP_K
):
    req_dict = {r.id: r for r in requirements}
    meth_dict = {m.id: m for m in methods}
    POSITIVE_LABELS = {"T", "E", "1", "TRUE"}

    def is_positive(t: dict) -> bool:
        lab = str(t.get("goldfinal", t.get("label", "F"))).upper()
        return lab in POSITIVE_LABELS

    def get_gold_for_req(req_name: str):
        gold_traces, gold_mids = [], set()
        for t in traces:
            if t.get("requirement") == req_name and is_positive(t):
                try:
                    mid = int(t.get("methodid"))
                except Exception:
                    continue
                gold_traces.append(t)
                gold_mids.add(mid)
        return gold_traces, gold_mids

    # --- –í—ñ–∫–Ω–æ ---
    root = tk.Tk()
    root.title("üîé Traceability GUI (TF-IDF / SBERT / CodeBERT + GOLD + –º–µ—Ç—Ä–∏–∫–∏)")
    # –æ–¥—Ä–∞–∑—É –Ω–∞ –≤–µ—Å—å –µ–∫—Ä–∞–Ω
    try:
        root.state("zoomed")    # Windows
    except Exception:
        root.attributes("-zoomed", True)  # Linux/macOS
    root.minsize(1200, 720)

    # –í–µ—Ä—Ö–Ω—è –ø–∞–Ω–µ–ª—å (GRID)
    control = ttk.Frame(root, padding=10)
    control.grid(row=0, column=0, sticky="ew")
    for c in range(9):
        control.grid_columnconfigure(c, weight=(0 if c in (0, 2, 4, 6) else 1))

    ttk.Label(control, text="ID –≤–∏–º–æ–≥–∏:").grid(row=0, column=0, sticky="w")
    entry_id = ttk.Entry(control, width=10)
    entry_id.grid(row=0, column=1, sticky="w", padx=(6, 12))

    ttk.Label(control, text="Top-K:").grid(row=0, column=2, sticky="w")
    entry_topk = ttk.Entry(control, width=7)
    entry_topk.insert(0, str(top_k_default))
    entry_topk.grid(row=0, column=3, sticky="w", padx=(6, 12))

    ttk.Label(control, text="–ú–æ–¥–µ–ª—å:").grid(row=0, column=4, sticky="w")
    model_var = tk.StringVar(value="CodeBERT")
    combo_model = ttk.Combobox(
        control,
        textvariable=model_var,
        values=list(retrieved_by_model.keys()),
        state="readonly",
        width=14
    )
    combo_model.grid(row=0, column=5, sticky="w", padx=(6, 12))

    btn_show = ttk.Button(control, text="–ü–æ–∫–∞–∑–∞—Ç–∏ –≤–∏–º–æ–≥—É")
    btn_show.grid(row=0, column=6, sticky="w")

    btn_global = ttk.Button(control, text="–ì–ª–æ–±–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏")
    btn_global.grid(row=0, column=7, sticky="w", padx=(10, 0))

    status_lbl = ttk.Label(control, text="", foreground="#555")
    status_lbl.grid(row=0, column=8, sticky="w", padx=(12, 0))

    # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏–π —Å–ø–ª—ñ—Ç
    paned = ttk.Panedwindow(root, orient="horizontal")
    paned.grid(row=1, column=0, sticky="nsew", padx=10, pady=(6, 6))

    left_frame = ttk.Frame(paned)
    right_frame = ttk.Frame(paned)
    paned.add(left_frame, weight=1)
    paned.add(right_frame, weight=1)

    left_txt = scrolledtext.ScrolledText(left_frame, width=80, height=25, wrap="word")
    left_txt.pack(fill="both", expand=True, padx=(0, 6), pady=0)

    right_txt = scrolledtext.ScrolledText(right_frame, width=80, height=25, wrap="word")
    right_txt.pack(fill="both", expand=True, padx=(6, 0), pady=0)

    # –î–µ—Ç–∞–ª—ñ
    detailed_txt = scrolledtext.ScrolledText(root, width=140, height=8, wrap="word")
    detailed_txt.grid(row=2, column=0, sticky="nsew", padx=10, pady=(0, 6))

    # –ü–æ–ª–æ—Ç–Ω–æ –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫–∞
    chart_frame = ttk.Frame(root)
    chart_frame.grid(row=3, column=0, sticky="nsew", padx=10, pady=(0, 10))

    fig, ax = plt.subplots(figsize=(9.2, 3.2))
    fig.tight_layout()
    canvas = FigureCanvasTkAgg(fig, master=chart_frame)
    canvas_widget = canvas.get_tk_widget()
    canvas_widget.pack(fill="both", expand=True)

    # –∞–¥–∞–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å
    root.grid_rowconfigure(1, weight=2)
    root.grid_rowconfigure(2, weight=1)
    root.grid_rowconfigure(3, weight=2)
    root.grid_columnconfigure(0, weight=1)

    # --- –ª–æ–≥—ñ–∫–∞ –ø–æ–∫–∞–∑—É –¥–ª—è –æ–¥–Ω—ñ—î—ó –≤–∏–º–æ–≥–∏ ---
    def show_methods():
        try:
            rid = int(entry_id.get().strip())
        except Exception:
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞", "–í–≤–µ–¥–∏ —Ü—ñ–ª–µ —á–∏—Å–ª–æ –¥–ª—è ID –≤–∏–º–æ–≥–∏.")
            return
        try:
            top_k = max(1, int(entry_topk.get().strip()))
        except Exception:
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞", "–í–≤–µ–¥–∏ —Ü—ñ–ª–µ —á–∏—Å–ª–æ –¥–ª—è Top-K.")
            return

        model_name = model_var.get()
        if model_name not in retrieved_by_model:
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞", f"–ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å: {model_name}")
            return

        retrieved_map = retrieved_by_model[model_name]

        req = req_dict.get(rid)
        if not req:
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞", f"–í–∏–º–æ–≥–∞ {rid} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return

        # –æ—á–∏—Å—Ç–∫–∞
        for w in (left_txt, right_txt, detailed_txt):
            w.config(state="normal")
            w.delete("1.0", tk.END)

        gold_traces, gold_method_ids = get_gold_for_req(req.requirementname)

        # –õ—ñ–≤–∞ –∫–æ–ª–æ–Ω–∫–∞: Top-K
        left_txt.insert(tk.END, f"üîπ –ú–æ–¥–µ–ª—å: {model_name}\n")
        left_txt.insert(tk.END, f"üîπ –í–∏–º–æ–≥–∞ {req.requirementname} (ID={req.id}):\n{req.text}\n\n")
        left_txt.insert(tk.END, f"üìä –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –º–µ—Ç–æ–¥–∏ (Top-{top_k}):\n")
        preds_for_req = retrieved_map.get(rid, [])[:top_k]
        for i, (mid, score) in enumerate(preds_for_req, 1):
            m = meth_dict.get(mid)
            name = (m.fullmethod.split('.')[-1] if (m and m.fullmethod) else str(mid))
            # trace_id –ª–∏—à–µ —è–∫—â–æ —î –ø–æ–∑–∏—Ç–∏–≤–Ω–∞ gold –ø–∞—Ä–∞ –¥–ª—è —Ü—å–æ–≥–æ methodid
            t = next((t for t in gold_traces if str(t.get("methodid")) == str(mid)), None)
            trace_id = t.get("id") if t else None
            if trace_id is not None:
                left_txt.insert(tk.END, f"{i}. method_id:{mid} trace_id:{trace_id} {name}  [{score:.4f}]\n")
            else:
                left_txt.insert(tk.END, f"{i}. method_id:{mid} {name}  [{score:.4f}]\n")

        # –ü—Ä–∞–≤–∞ –∫–æ–ª–æ–Ω–∫–∞: —Ç—ñ–ª—å–∫–∏ GOLD
        right_txt.insert(tk.END, "üìò GOLD (–ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Ç—Ä–µ—Å–∏ –¥–ª—è —Ü—ñ—î—ó –≤–∏–º–æ–≥–∏):\n")
        if not gold_traces:
            right_txt.insert(tk.END, "‚Äî –Ω–µ–º–∞—î –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ç–æ–∫ ‚Äî\n")
        else:
            right_txt.insert(tk.END, f"(—É—Å—å–æ–≥–æ: {len(gold_traces)})\n")
            for t in sorted(gold_traces, key=lambda x: int(x.get("methodid", 0))):
                mid = int(t.get("methodid"))
                m = meth_dict.get(mid)
                name = (m.fullmethod.split('.')[-1] if (m and m.fullmethod) else str(mid))
                lab = str(t.get('goldfinal', t.get('label', ''))).upper()
                right_txt.insert(
                    tk.END,
                    f"req:{req.requirementname} | trace_id:{t.get('id')} | methodid:{mid} | {name} | label:{lab}\n"
                )

        # –î–µ—Ç–∞–ª—ñ + –º–µ—Ç—Ä–∏–∫–∏
        pred_method_ids = [mid for (mid, _) in preds_for_req]
        hits = sum(1 for pid in pred_method_ids if pid in gold_method_ids)
        precision_pct = (hits / len(pred_method_ids) * 100.0) if pred_method_ids else 0.0

        detailed_txt.insert(tk.END, f"üìù –î–µ—Ç–∞–ª—å–Ω–∏–π –≤–∏–≤—ñ–¥ –¥–ª—è {req.requirementname} (–º–æ–¥–µ–ª—å {model_name}, Top-{top_k}):\n")
        for i, (mid, score) in enumerate(preds_for_req, 1):
            m = meth_dict.get(mid)
            name = (m.fullmethod.split('.')[-1] if (m and m.fullmethod) else str(mid))
            is_gold = "‚úì" if mid in gold_method_ids else " "
            detailed_txt.insert(
                tk.END,
                f"{i:>2}. [{is_gold}] req:{req.requirementname}  method_id:{mid}  {name}  [score={score:.4f}]\n"
            )
        detailed_txt.insert(
            tk.END,
            f"\n‚úÖ –ó–±—ñ–≥—ñ–≤: {hits} / {len(pred_method_ids)} | –¢–æ—á–Ω—ñ—Å—Ç—å (–ø–æ Top-{top_k}): {precision_pct:.2f}%\n"
        )

        # –õ–æ–∫–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        metrics = compute_local_metrics(pred_method_ids, gold_method_ids)
        for k, v in metrics.items():
            detailed_txt.insert(tk.END, f"  {k}: {v:.4f}\n")

        # –ì—Ä–∞—Ñ—ñ–∫
        ax.clear()
        keys = list(metrics.keys())
        vals = [metrics[k] for k in keys]
        bars = ax.bar(keys, vals)
        ax.set_ylim(0, 1)
        ax.set_ylabel("Score")
        ax.set_title(f"–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {req.requirementname} (–º–æ–¥–µ–ª—å {model_name})")
        for b, val in zip(bars, vals):
            ax.text(
                b.get_x() + b.get_width() / 2,
                b.get_height() + 0.02,
                f"{val:.2f}",
                ha="center",
                va="bottom",
                fontsize=9,
            )
        for label in ax.get_xticklabels():
            label.set_rotation(12)
            label.set_ha("right")
        canvas.draw()

        # —Å—Ç–∞—Ç—É—Å
        if not preds_for_req:
            status_lbl.config(text="–ù–µ–º–∞—î –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–ª—è —Ü—ñ—î—ó –≤–∏–º–æ–≥–∏.")
        elif not gold_method_ids:
            status_lbl.config(text="–£ GOLD –Ω–µ–º–∞—î –ø–æ–∑–∏—Ç–∏–≤—ñ–≤ –¥–ª—è —Ü—ñ—î—ó –≤–∏–º–æ–≥–∏.")
        else:
            status_lbl.config(text=f"–ú–æ–¥–µ–ª—å: {model_name}")

    # --- –≥–ª–æ–±–∞–ª—å–Ω—ñ (—Å–µ—Ä–µ–¥–Ω—ñ) –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å—ñ—Ö –≤–∏–º–æ–≥–∞—Ö –¥–ª—è –æ–±—Ä–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ ---
    def show_global_metrics():
        for w in (left_txt, right_txt, detailed_txt):
            w.config(state="normal")
            w.delete("1.0", tk.END)

        model_name = model_var.get()
        if model_name not in retrieved_by_model:
            detailed_txt.insert(tk.END, f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å: {model_name}\n")
            status_lbl.config(text="–ü–æ–º–∏–ª–∫–∞ –º–æ–¥–µ–ª—ñ.")
            return

        retrieved_map = retrieved_by_model[model_name]

        try:
            top_k = max(1, int(entry_topk.get().strip()))
        except Exception:
            top_k = top_k_default

        sum_metrics = {
            "Precision": 0.0,
            "Recall": 0.0,
            "F1": 0.0,
            "MAP": 0.0,
            "MRR": 0.0,
        }
        cnt = 0

        for r in requirements:
            gold_ids = gold_truth.get(r.requirementname, set())
            pred_pairs = retrieved_map.get(r.id, [])
            pred_ids = [mid for (mid, _) in pred_pairs[:top_k]]
            if not pred_ids or not gold_ids:
                continue
            m = compute_local_metrics(pred_ids, gold_ids)
            for k in sum_metrics:
                sum_metrics[k] += m[k]
            cnt += 1

        if cnt == 0:
            detailed_txt.insert(tk.END, "‚ùå –ù–µ–º–∞—î –≤–∏–º–æ–≥ –∑ gold-—Ç—Ä–∞—Å–∞–º–∏ –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫.\n")
            status_lbl.config(text="–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫.")
            return

        avg = {k: v / cnt for k, v in sum_metrics.items()}

        detailed_txt.insert(
            tk.END,
            f"üìä –ì–ª–æ–±–∞–ª—å–Ω—ñ (—É—Å–µ—Ä–µ–¥–Ω–µ–Ω—ñ) –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å—ñ—Ö –≤–∏–º–æ–≥–∞—Ö –¥–ª—è –º–æ–¥–µ–ª—ñ {model_name} (Top-{top_k}):\n"
        )
        detailed_txt.insert(tk.END, f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–º–æ–≥ –∑ gold-—Ç—Ä–∞—Å–∞–º–∏: {cnt}\n\n")
        for k, v in avg.items():
            detailed_txt.insert(tk.END, f"{k}: {v:.4f}\n")

        # –≥—Ä–∞—Ñ—ñ–∫
        ax.clear()
        keys = list(avg.keys())
        vals = [avg[k] for k in keys]
        bars = ax.bar(keys, vals)
        ax.set_ylim(0, 1)
        ax.set_ylabel("Score")
        ax.set_title(f"–ì–ª–æ–±–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ (–º–æ–¥–µ–ª—å {model_name})")
        for b, val in zip(bars, vals):
            ax.text(
                b.get_x() + b.get_width() / 2,
                b.get_height() + 0.02,
                f"{val:.2f}",
                ha="center",
                va="bottom",
                fontsize=9,
            )
        for label in ax.get_xticklabels():
            label.set_rotation(12)
            label.set_ha("right")
        canvas.draw()

        status_lbl.config(text=f"–ì–ª–æ–±–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—á–∏—Å–ª–µ–Ω–æ (–º–æ–¥–µ–ª—å {model_name}).")

    btn_show.configure(command=show_methods)
    btn_global.configure(command=show_global_metrics)

    # –∫–æ—Ä–µ–∫—Ç–Ω–µ –∑–∞–∫—Ä–∏—Ç—Ç—è
    def on_close():
        try:
            canvas_widget.destroy()
        except Exception:
            pass
        try:
            plt.close(fig)
        except Exception:
            pass
        try:
            root.quit()
            root.destroy()
        except Exception:
            pass

    root.protocol("WM_DELETE_WINDOW", on_close)
    root.bind("<Escape>", lambda e: on_close())
    root.mainloop()

# =======================
# Main
# =======================
if __name__ == "__main__":
    if faiss is None:
        USE_FAISS = False

    methods, requirements, gold_truth, traces = load_dataset(DATA_DIR)

    retrieved_by_model: Dict[str, Dict[int, List[Tuple[int, float]]]] = {}
    metrics_by_model: Dict[str, Dict[str, float]] = {}

    # –ü–æ—Ä—è–¥–æ–∫: TF-IDF ‚Üí SBERT ‚Üí CodeBERT
    for model_name in ["TF-IDF", "SBERT", "CodeBERT"]:
        try:
            retrieved, avg_metrics = run_model_pipeline(
                model_name,
                methods,
                requirements,
                gold_truth,
                TOP_K
            )
            retrieved_by_model[model_name] = retrieved
            metrics_by_model[model_name] = avg_metrics
        except ImportError as e:
            print(f"\n‚ùå –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º–æ–¥–µ–ª—å {model_name}: {e}")
        except Exception as e:
            print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_name}: {e}")

    # –ö–æ—Ä–æ—Ç–∫–µ —Ä–µ–∑—é–º–µ –¥–ª—è –¢–∞–±–ª–∏—Ü—ñ 4.1
    print("\n==========================")
    print("   –ü–Ü–î–°–£–ú–ö–û–í–ê –¢–ê–ë–õ–ò–¶–Ø (–¥–ª—è –º–∞–≥—ñ—Å—Ç–µ—Ä—Å—å–∫–æ—ó)")
    print("==========================")
    header = f"{'–ú–æ–¥–µ–ª—å':10s} | {'Precision':9s} | {'Recall':7s} | {'F1-score':8s} | {'MAP':6s} | {'MRR':6s}"
    print(header)
    print("-" * len(header))
    for model_name in ["TF-IDF", "SBERT", "CodeBERT"]:
        m = metrics_by_model.get(model_name)
        if not m:
            continue
        print(
            f"{model_name:10s} | "
            f"{m['Precision']:9.4f} | "
            f"{m['Recall']:7.4f} | "
            f"{m['F1']:8.4f} | "
            f"{m['MAP']:6.4f} | "
            f"{m['MRR']:6.4f} "
        )

    # –ó–∞–ø—É—Å–∫ GUI –∑ –ø–µ—Ä–µ–º–∏–∫–∞—á–µ–º –º–æ–¥–µ–ª–µ–π
    if retrieved_by_model:
        launch_gui(requirements, methods, gold_truth, traces, retrieved_by_model, TOP_K)
    else:
        print("‚ùå –ñ–æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—É–ª–∞ —É—Å–ø—ñ—à–Ω–æ –ø–æ–±—É–¥–æ–≤–∞–Ω–∞ ‚Äì GUI –Ω–µ –∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è.")
